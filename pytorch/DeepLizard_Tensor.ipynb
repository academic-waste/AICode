{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f13474a-3dde-4c67-b802-3004c050116f",
   "metadata": {},
   "source": [
    "### pytorch张量及其创建\n",
    "- 在神经网络中第一步就是对数据进行预处理\n",
    "- 预处理的目的就是为了将我们拿到的数据转换成能被神经网络感知的张量\n",
    "\n",
    "#### 创建张量\n",
    "- pytorch提供四种方式创建张量\n",
    "- 工厂函数：接受参数输入并返回特定类型对象的函数；dtype\n",
    "- 通常情况下会更倾向于选择工厂函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e198348d-3f98-40c5-97e8-e48df59fc7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "# 创建张量\n",
    "t = np.array([[1,2,3],[4,5,6],[7,8,9]]) # 创建一个ndarray\n",
    "print(torch.Tensor(t))           # 类构造函数\n",
    "print(torch.tensor(t))           # 工厂函数\n",
    "print(torch.as_tensor(t))        # 工厂函数\n",
    "print(torch.from_numpy(t))       # 工厂函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294a861-e0da-4e9f-807a-cc93afec4cd6",
   "metadata": {},
   "source": [
    "#### 4种创建张量的区别1 - 数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b914af6c-936c-4b1a-98bd-e9c9100e4f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.float32\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "t1 = torch.Tensor(data)\n",
    "print(t1)\n",
    "print(t1.dtype)\n",
    "t2 = torch.tensor(data)\n",
    "print(t2)\n",
    "print(t2.dtype)\n",
    "t3 = torch.as_tensor(data)\n",
    "print(t3)\n",
    "print(t3.dtype)\n",
    "t4 = torch.from_numpy(data)\n",
    "print(t4)\n",
    "print(t4.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37938ef-e6f2-4512-b056-de8b5688264e",
   "metadata": {},
   "source": [
    "- 使用类构造函数和工厂函数后生成的数据类型不同，主要原因是：构造函数在构造一个张量时使用全局缺省值，而工厂函数通过输入数据的类型来推断输出数据的类型\n",
    "- 通过torch.get_default_dtype 获得全局默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a1c6a1a1-a96c-4fdc-a6b0-9efac3f2904b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default type  torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"default type \",torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50fe71-155a-484d-9a26-b34452374718",
   "metadata": {},
   "source": [
    "- 显示指定数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e509fc-6f19-4fec-a1db-43df89bafa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1,2],[3,4]],dtype=torch.float32)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328c150-d14d-4566-b37a-bcfe438371e5",
   "metadata": {},
   "source": [
    " #### 四种方法的区别2：数据分配内存方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0f1e036f-054f-4d26-98ac-a252d48bfa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m t2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data)\n\u001b[1;32m      5\u001b[0m t3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m t4 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(t1,t2,t3,t4)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# update value in data, all elements to 0s\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "print(data)\n",
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)\n",
    "print(t1,t2,t3,t4)\n",
    "\n",
    "# update value in data, all elements to 0s\n",
    "data[1]=0\n",
    "data[2]=0\n",
    "# t1 和 t2 输出的都是更改前的数组\n",
    "print(t1)\n",
    "print(t2)\n",
    "# t3 和 t4 输出的都是更改后的数组\n",
    "print(t3)\n",
    "print(t4)\n",
    "print('********')\n",
    "data[2] = 12.2\n",
    "print(data)\n",
    "print(t3)\n",
    "print(t4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f36d66-41d6-478c-8b25-edc38a562ba0",
   "metadata": {},
   "source": [
    "- 上述差异由创建tensor时内存分配造成。\n",
    "- t1 t2在创建时会复制数组中的元素到内存，因此改变data的值不会影响tensor\n",
    "- t3 t4数据与data内存共享，因此当元数据被修改后t3 t4同样被修改\n",
    "- 我们可以将t1 t2 当做“值传递”，t3 t4当做“引用传递”\n",
    "- 数据共享比数据拷贝更加高效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c34569-eae8-4754-bc37-9b42f1050343",
   "metadata": {},
   "source": [
    "#### np->tensor最佳转换方式\n",
    "- 数据拷贝最佳方式 - torch.tensor()\n",
    "- 数据共享最佳方式 - torch.as_tensor()，因为as_tensor()可接收任意类型的参数，而from_numpy只接收ndarray类型的数据\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c55bc1-81bb-49b4-8743-b65c4ddc7231",
   "metadata": {},
   "source": [
    "#### 其他方式创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f4a61233-c4d5-4d36-9ebf-aade33f59403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([[[ 3,  7],\n",
      "         [16,  6],\n",
      "         [18, 10]],\n",
      "\n",
      "        [[16,  3],\n",
      "         [11, 17],\n",
      "         [ 6, 14]],\n",
      "\n",
      "        [[ 3,  2],\n",
      "         [19, 17],\n",
      "         [ 8, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# 单位张量的创建(二维),dtype=torch.float)\n",
    "eye = torch.eye(4,3,dtype=torch.float)\n",
    "print(eye)\n",
    "#全零张量的创建（二维）\n",
    "zeros = torch.zeros_like(eye)\n",
    "print(zeros)\n",
    "#全1张量的创建(二维)\n",
    "ones = torch.ones(4,5) \n",
    "print(ones )\n",
    "\n",
    "ar = torch.arange(1,10,2)\n",
    "print(ar)\n",
    "\n",
    "int_t = torch.randint(0,20,(3,3,2))\n",
    "print(int_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8deecb3-9bcb-42f1-9a91-89052cb4db32",
   "metadata": {},
   "source": [
    "### Tensor属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4a824ca7-8992-4509-8820-3ae7e65c8f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6812, -0.8365, -0.2817,  0.5858],\n",
      "        [ 0.6637,  1.0637, -0.0299,  1.2013],\n",
      "        [ 1.2485,  1.2558, -0.3473, -0.1321]])\n",
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n",
      "2\n",
      "12\n",
      "False\n",
      "None\n",
      "0.004718895156095535\n",
      "tensor(1.9975)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 Tensor\n",
    "tensor = torch.randn(3, 4)\n",
    "print(tensor)\n",
    "# 查看 Tensor 的形状\n",
    "print(tensor.shape)  # (3, 4)\n",
    "\n",
    "# 查看 Tensor 的数据类型\n",
    "print(tensor.dtype)  # torch.float32\n",
    "\n",
    "# 查看 Tensor 所在的设备\n",
    "print(tensor.device)  # cpu\n",
    "\n",
    "# 查看 Tensor 的维度数\n",
    "print(tensor.ndim)  # 2\n",
    "\n",
    "# 查看 Tensor 中元素的总数\n",
    "print(tensor.numel())  # 12\n",
    "\n",
    "# 查看 Tensor 是否需要梯度计算\n",
    "print(tensor.requires_grad)  # False\n",
    "print(tensor.grad)\n",
    "\n",
    "\n",
    "n = np.random.randn(300,400)\n",
    "print(n.mean())\n",
    "\n",
    "t = torch.randn(300,400)\n",
    "\n",
    "print(t.mean().add(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac89ea-086a-4625-9f8b-6b4d1885ceaf",
   "metadata": {},
   "source": [
    "#### 数值类型:\n",
    "\n",
    "- 浮点数:\n",
    "    - torch.float32: 32 位浮点数，通常用于存储实数数据。\n",
    "    - torch.float64: 64 位浮点数，通常用于存储高精度实数数据。\n",
    "- 整数:\n",
    "    - torch.int8: 8 位整数，通常用于存储较小的整数数据。\n",
    "    - torch.int16: 16 位整数，通常用于存储中等大小的整数数据。\n",
    "    - torch.int32: 32 位整数，通常用于存储较大的整数数据。\n",
    "    - torch.int64: 64 位整数，通常用于存储高精度整数数据\n",
    "\n",
    "\n",
    "- 选择数据类型时，应考虑以下因素:\n",
    "\n",
    "    - 精度: 所需的精度决定了应使用哪种数据类型。例如，如果要进行科学计算，则需要使用高精度数据类型，例如 torch.float64。\n",
    "    - 存储空间: 不同数据类型占用不同的存储空间。例如，torch.float64 数据类型占用比 torch.float32 数据类型更多的存储空间。\n",
    "    - 计算效率: 不同数据类型具有不同的计算效率。例如，torch.float32 数据类型的计算效率通常高于 torch.float64 数据类型。\n",
    "- 常用的数据类型\n",
    "    - 图像: 通常使用 torch.uint8 数据类型存储图像，因为每个像素值介于 0 和 255 之间。\n",
    "    - 文本: 通常使用 torch.string 数据类型存储文本。\n",
    "    - 实数: 通常使用 torch.float32 数据类型存储实数，因为它具有足够的精度和计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4b3b6-321c-4c13-a403-bbd6d2ee3c3d",
   "metadata": {},
   "source": [
    "### Tensor的操作\n",
    "- 常见的张量操作类型\n",
    "    - Reshaping operations    -- 重塑\n",
    "    - Element-wise operations -- 元素级操作\n",
    "    - Reduction operations    -- 缩减操作\n",
    "    - Access operations       -- 访问操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce2fdd-9e55-471c-a453-a247b256a90d",
   "metadata": {},
   "source": [
    "#### Reshaping operations\n",
    "- 重塑是Tensor最重要的操作之一， 通过张量的重塑我们可以将数据转换为符合特定要求的操作。例如在卷积神经网络中，需要将输入图像reshape成适合卷积操作的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8079834-8308-41bf-9a04-f8850d4a37f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor(12)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1,1,1,1],\n",
    "                 [2,2,2,2],\n",
    "                  [3,3,3,3]], dtype=torch.float32)\n",
    "# print(t.size())\n",
    "# 获得张量形状(3,4)\n",
    "print(t.shape) \n",
    "\n",
    "# 获得tensor的元素个数\n",
    "print(torch.tensor(t.shape).prod())\n",
    "print(t.numel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e3aa1-b2de-49e5-9871-cf9e8f15738a",
   "metadata": {},
   "source": [
    "#### Reshape\n",
    "- 有时候我们想要彻底改变张量的形状，但同时仍然保留元素的数量及其内容。\n",
    "- 这种情况一般会在CNN模型的卷积层和模型的线性层之间的接口处\n",
    "- 这在图像分类模型中很常见。卷积核将产生 x 宽度 y高度的输出张量，但后面的线性层需要一维输入。reshape()便可以执行该操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d4be8330-8fe4-4060-8b6a-8205c52bd73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([2352])\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(3,28,28)\n",
    "print(t.shape)\n",
    "\n",
    "t1 = t.reshape(3*28*28) #\n",
    "print(t1.shape)\n",
    "\n",
    "t2 = torch.reshape(t1,(3,28,28))\n",
    "print(t2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ae332-d51a-4f3b-a4d9-5eebaa9c3966",
   "metadata": {},
   "source": [
    "#### squeeze/unsqueeze\n",
    "- squeeze/unsqueeze用来修改张量维度数\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6e940e4e-bfc3-42bd-a358-231114cc19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 1, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# unqueeze\n",
    "# 假设存在一个表示图片信息的张量(3,28,28).此时模型需要输入批次信息，那么我们可以使用unsequeeze方式来新增一个批次维度\n",
    "t = torch.rand(3,28,28)\n",
    " \n",
    "t1 = torch.unsqueeze(t,-4) # torch.unsqueeze(t,1/2/3) \n",
    "print(t1.shape)\n",
    "print(t1.shape)\n",
    "t2=torch.unsqueeze(t1,-4) # torch.unsqueeze(t,1/2/3) \n",
    "print(t2.shape)\n",
    "# 通过unsqueeze(t,0) 在原始张量新增了一个维度 第0维\n",
    "# 维度的新增不会改变张量元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c0c3d319-37f9-40c9-a30a-d9290ae54cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 10])\n",
      "tensor([[[2.3860e-01, 6.2105e-01, 8.8141e-01, 3.3826e-01, 3.4979e-01,\n",
      "          1.8168e-01, 2.0700e-01, 8.8389e-01, 5.4165e-01, 6.7819e-01],\n",
      "         [3.7341e-02, 2.2192e-01, 8.7680e-01, 5.4235e-01, 3.5240e-01,\n",
      "          7.4247e-01, 7.3136e-01, 4.4930e-01, 1.3222e-01, 8.9691e-01],\n",
      "         [1.2803e-01, 1.9953e-01, 9.6692e-01, 3.9269e-01, 6.3754e-01,\n",
      "          8.8554e-01, 6.6381e-01, 9.6527e-02, 4.7500e-02, 3.4489e-01]],\n",
      "\n",
      "        [[7.8327e-01, 5.9071e-01, 3.2326e-01, 4.0202e-01, 7.9513e-01,\n",
      "          2.4478e-01, 4.9333e-01, 3.7004e-01, 5.9928e-01, 5.1452e-01],\n",
      "         [8.5287e-01, 8.4385e-03, 7.4425e-01, 2.6430e-01, 8.3342e-01,\n",
      "          8.9788e-01, 8.2888e-01, 7.8374e-01, 8.9340e-01, 4.5597e-01],\n",
      "         [4.1275e-01, 4.6898e-01, 7.1635e-01, 5.8191e-01, 4.6249e-02,\n",
      "          1.9194e-01, 5.3408e-01, 8.2432e-01, 8.4030e-01, 2.6900e-04]]])\n",
      "torch.Size([2, 3, 10])\n",
      "tensor([[[2.3860e-01, 6.2105e-01, 8.8141e-01, 3.3826e-01, 3.4979e-01,\n",
      "          1.8168e-01, 2.0700e-01, 8.8389e-01, 5.4165e-01, 6.7819e-01],\n",
      "         [3.7341e-02, 2.2192e-01, 8.7680e-01, 5.4235e-01, 3.5240e-01,\n",
      "          7.4247e-01, 7.3136e-01, 4.4930e-01, 1.3222e-01, 8.9691e-01],\n",
      "         [1.2803e-01, 1.9953e-01, 9.6692e-01, 3.9269e-01, 6.3754e-01,\n",
      "          8.8554e-01, 6.6381e-01, 9.6527e-02, 4.7500e-02, 3.4489e-01]],\n",
      "\n",
      "        [[7.8327e-01, 5.9071e-01, 3.2326e-01, 4.0202e-01, 7.9513e-01,\n",
      "          2.4478e-01, 4.9333e-01, 3.7004e-01, 5.9928e-01, 5.1452e-01],\n",
      "         [8.5287e-01, 8.4385e-03, 7.4425e-01, 2.6430e-01, 8.3342e-01,\n",
      "          8.9788e-01, 8.2888e-01, 7.8374e-01, 8.9340e-01, 4.5597e-01],\n",
      "         [4.1275e-01, 4.6898e-01, 7.1635e-01, 5.8191e-01, 4.6249e-02,\n",
      "          1.9194e-01, 5.3408e-01, 8.2432e-01, 8.4030e-01, 2.6900e-04]]])\n"
     ]
    }
   ],
   "source": [
    "# squeeze\n",
    "# 假如模型返回的数据是[1,10]的矩阵数据，但是我们期望的返回是一个只有10个数字的一维向量。 此时，我们可以通过sequeeze()方法来实现。去删掉一个长度为1的秩删掉\n",
    "\n",
    "t = torch.rand(2,3,10)\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "t1 = torch.squeeze(t,1) # 如果不给index， 则会将所有长度为1的秩删掉\n",
    "print(t1.shape)\n",
    "print(t1)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97725796-3950-484e-8c64-c972746fbb20",
   "metadata": {},
   "source": [
    "#### flatten\n",
    "- flatten张量：除去所有的轴，只保留一个，创造一个单轴张量包含原张量所有元素；\n",
    "- flatten操作是从一个卷积层过度到一个全连接层时在神经网络中必须发生的；\n",
    "- flatten操作是一种特殊的reshaping操作，即所有轴被挤压成一个轴\n",
    "- flatten操作的前提：张量至少有两个轴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f41c5da3-1873-4588-ae84-082754b751da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4137, 0.7878],\n",
      "         [0.0748, 0.8946]],\n",
      "\n",
      "        [[0.7580, 0.3687],\n",
      "         [0.8001, 0.2260]]])\n",
      "torch.Size([8])\n",
      "tensor([0.4137, 0.7878, 0.0748, 0.8946, 0.7580, 0.3687, 0.8001, 0.2260])\n",
      "torch.Size([8])\n",
      "tensor([0.4137, 0.7878, 0.0748, 0.8946, 0.7580, 0.3687, 0.8001, 0.2260])\n",
      "torch.Size([8])\n",
      "tensor([0.4137, 0.7878, 0.0748, 0.8946, 0.7580, 0.3687, 0.8001, 0.2260])\n",
      "tensor([0.4137, 0.7878, 0.0748, 0.8946, 0.7580, 0.3687, 0.8001, 0.2260])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2,2,2)\n",
    "print(t)\n",
    "# flatten\n",
    "t_flatten = torch.flatten(t)\n",
    "print(t_flatten.shape)\n",
    "print(t_flatten)\n",
    "# reshape\n",
    "t_reshape =  t.reshape(t.numel())\n",
    "print(t_reshape.shape)\n",
    "print(t_reshape)\n",
    "# reshape  -1 \n",
    "t_reshape_1 =  t.reshape(-1)\n",
    "print(t_reshape_1.shape)\n",
    "print(t_reshape_1)\n",
    "\n",
    "t_view= t.view(t.numel())\n",
    "print(t_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8de91d-ce04-418d-bb6e-aaa8c4aef7cc",
   "metadata": {},
   "source": [
    "#### 转置\n",
    "- PyTorch 中的转置操作用于改变张量的维度顺序，这在卷积、矩阵乘法都可能被使用到。\n",
    "- torch.transpose() \n",
    "- t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2ac5abdb-0abd-4a5d-a708-53c887bcbb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 6])\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "tensor([[0.5234, 0.4073],\n",
      "        [0.4301, 0.2158],\n",
      "        [0.4927, 0.9602]])\n",
      "tensor([[0.5234, 0.4073],\n",
      "        [0.4301, 0.2158],\n",
      "        [0.4927, 0.9602]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#a2 = torch.transpose(a2,0,1)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(a2)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m tt\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(tt)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# transpose()\n",
    "'''\n",
    "参数说明\n",
    "    input (Tensor): 需要转置的张量\n",
    "    dim0 (int): 要转置的第一个维度\n",
    "    dim1 (int): 要转置的第二个维度\n",
    "    默认情况下，dim0 为 0，dim1 为 1，这意味着将第一个维度和第二个维度交换。\n",
    "    可以使用 torch.transpose(input, dim0, dim1) 来指定要交换的维度。\n",
    "'''\n",
    "x = torch.arange(9).reshape(3, 3)\n",
    "'''\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8]])\n",
    "       '''\n",
    "'''\n",
    "        [[0, 3, 6],\n",
    "        [1, 4, 7],\n",
    "        [2, 5, 8]]\n",
    "'''\n",
    "print(x[:,0])\n",
    "y = torch.transpose(x,1,0)\n",
    "print(y)\n",
    "\n",
    "z = x.t()\n",
    "\n",
    "print(z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4ab45931-c01f-4fe1-8fe0-8ddc69dccfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0085, 0.1911],\n",
      "        [0.2797, 0.2603],\n",
      "        [0.7056, 0.1655]])\n",
      "tensor([[0.0085, 0.2797, 0.7056],\n",
      "        [0.1911, 0.2603, 0.1655]])\n",
      "tensor([[0.0080, 0.1060, 0.3953],\n",
      "        [0.0443, 0.1771, 0.1447]])\n"
     ]
    }
   ],
   "source": [
    "# 例如我们存在2个张量， 要讲他们相乘\n",
    "a1 = torch.rand(2,3)\n",
    "a2 = torch.rand(3,2)\n",
    "print(a2)\n",
    "a2 = torch.transpose(a2,0,1)\n",
    "print(a2)\n",
    "print(torch.multiply(a1,a2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ec3c1bf8-9cf0-4237-a4a2-08693044a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [13, 14, 15, 16]],\n",
      "\n",
      "        [[ 5,  6,  7,  8],\n",
      "         [17, 18, 19, 20]],\n",
      "\n",
      "        [[ 9, 10, 11, 12],\n",
      "         [21, 22, 23, 24]]])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "tt= torch.arange(1,25).reshape(2,3,4)\n",
    "print(tt)\n",
    "\n",
    "print(tt.shape)\n",
    "tt1 = torch.transpose(tt,0,1)\n",
    "print(tt1)\n",
    "print(tt1.shape)\n",
    "# tt2 = tt.permute(0,1,2)\n",
    "#print(tt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c346740-7fef-4591-846f-d3793ccae344",
   "metadata": {},
   "source": [
    "#### 张量的元素运算(element-wise operation)\n",
    "- 元素运算是对张量元素的运算，这些元素在张量中对应或具有相同的位置索引\n",
    "- 两个张量必须有相同的形状才能执行元素操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ed2270b-90a4-4f56-a1e9-4d226a507a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5, 7],\n",
      "        [3, 5, 7]])\n",
      "tensor([[3, 5, 7],\n",
      "        [3, 5, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# 加法  \n",
    "x = torch.tensor([[1,2,3],[1,2,3]])\n",
    "y = torch.tensor([[2,3,4],[2,3,4]])\n",
    "print(x+y)\n",
    "print(torch.add(x,y))\n",
    "# 支持广播\n",
    "print(x+1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "009110b2-65fb-4a8f-8a29-30b70d0621d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "tensor([[-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "# 减法\n",
    "x = torch.tensor([[1,2,3],[1,2,3]])\n",
    "y = torch.tensor([[2,3,4],[2,3,4]])\n",
    "print(x-y)\n",
    "print(torch.sub(x,y))\n",
    "# 支持广播\n",
    "print(y-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4646055-eb09-45ad-b680-ae4e6797ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  9, 12],\n",
      "        [ 4,  6, 16]])\n",
      "tensor([[ 2,  9, 12],\n",
      "        [ 4,  6, 16]])\n",
      "tensor([[14, 21, 28],\n",
      "        [16, 24, 32]])\n",
      "tensor([[14, 21, 28],\n",
      "        [16, 24, 32]])\n"
     ]
    }
   ],
   "source": [
    "# 乘法\n",
    "x = torch.tensor([[1,3,3],[2,2,4]])\n",
    "y = torch.tensor([[2,3,4],[2,3,4]])\n",
    "z = torch.tensor([[2,3,4],[2,3,4],[2,3,4]])\n",
    "print(x * y )\n",
    "print(torch.multiply(x,y))\n",
    "\n",
    "print(torch.mm(x,z))\n",
    "print(torch.matmul(x,z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0b63e-f03c-496e-8f1f-e1dfd49d76d2",
   "metadata": {},
   "source": [
    "#### broadcast\n",
    "- 规则\n",
    "    - 两个张量在每个维度上至少有一个维度大小相同。\n",
    "    - 较小张量在不改变数据的情况下，根据规则扩展到与较大张量相同形状。\n",
    "    - 扩展后的两个张量进行逐元素运算。\n",
    "\n",
    "- 广播机制只适用于形状兼容的张量。如果两个张量形状不匹配，则会抛出错误。\n",
    "- 使用广播机制时，应注意数据类型是否兼容。\n",
    "- 广播机制可能会降低计算效率，应尽量避免使用不必要的广播运算。\n",
    "\n",
    "- 运算步骤\n",
    "    - 标量与张量\n",
    "      - 标量 5 与形状为 (2, 3) 的张量进行加法运算。\n",
    "      - 标量 5 被扩展到与张量相同形状，即 (2, 3)。\n",
    "      - 扩展后的标量与张量进行逐元素相加。\n",
    "      - 最终结果为形状为 (2, 3) 的张量。\n",
    "    - 部分维度相同的张量\n",
    "      - 形状为 (2, 1) 的张量 A 与形状为 (2, 3) 的张量 B 进行加法运算。\n",
    "      - 两个张量在第一维度上大小相同，因此在该维度上进行逐元素相加\n",
    "      - 在第二维度上，张量 A 大小为 1，因此将其扩展到与张量 B 在该维度上相同大小，即 3。\n",
    "      - 扩展后的张量 A 与张量 B 进行逐元素相加。\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "649c1073-cd36-4f2a-bcb8-42cc43704dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0525, 1.7336, 1.5868, 0.9805],\n",
       "        [0.7200, 0.5233, 0.1767, 0.4708]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.rand(1)\n",
    "t1 = torch.rand(2,1)\n",
    "t2 = torch.rand(2,4)\n",
    "t+t1\n",
    "t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "a41dd8da-0d64-4b95-b7a5-75b266093abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 2., 3.],\n",
      "        [1., 2., 4.]], grad_fn=<DivBackward0>)\n",
      "tensor([[4., 2., 3.],\n",
      "        [1., 2., 4.]], grad_fn=<DivBackward0>)\n",
      "Next Course\n",
      "tensor([[2.3393, 0.6079, 1.3809],\n",
      "        [0.1057, 0.3731, 2.0120]])\n",
      "tensor([[-1.2515, -0.6373, -0.9603],\n",
      "        [-0.2724, -0.5199, -1.1697]])\n"
     ]
    }
   ],
   "source": [
    "# 除法\n",
    "\n",
    "x = torch.tensor([[8,10,12],[2,2,4]],requires_grad=True,dtype=torch.float)\n",
    "y = torch.tensor([[2,5,4],[2,1,1]],requires_grad=True,dtype=torch.float)\n",
    "print(x / y )\n",
    "\n",
    "\n",
    "res = torch.div(x,y)\n",
    "\n",
    "print(res)\n",
    "\n",
    "\n",
    "print('Next Course')\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(res,x)\n",
    "\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f4417-2795-4126-a058-69a65883f33f",
   "metadata": {},
   "source": [
    "#### Reduction operations -- 缩减操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9de3d4fd-30cd-4bcb-b366-5e69634146ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1.,   2., 100.,   4.],\n",
      "         [  5.,   6.,   7.,   8.],\n",
      "         [  9.,  10.,  11.,  12.]],\n",
      "\n",
      "        [[ 13.,  14.,  15.,  16.],\n",
      "         [ 17.,  18.,  19.,  20.],\n",
      "         [ 21.,  22.,  23.,  24.]]])\n",
      "torch.return_types.max(\n",
      "values=tensor([[  9.,  10., 100.,  12.],\n",
      "        [ 21.,  22.,  23.,  24.]]),\n",
      "indices=tensor([[2, 2, 0, 2],\n",
      "        [2, 2, 2, 2]]))\n",
      "tensor(2)\n",
      "tensor(16.5417)\n",
      "tensor(19.0240)\n",
      "tensor(2.0682e+25)\n",
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "t= torch.arange(1,25).reshape(2,3,4)\n",
    "t=t.to(torch.float)\n",
    "t[0][0][2]=100\n",
    "print(t)\n",
    "\n",
    "\n",
    "\n",
    "# max value\n",
    "print(torch.max(t,axis=1))\n",
    "# max value index\n",
    "print(torch.argmax(t))\n",
    "\n",
    "# average \n",
    "print(torch.mean(t)) # \n",
    "\n",
    "# std standard deviation\n",
    "print(torch.std(t))\n",
    "\n",
    "# product all elements\n",
    " \n",
    "print(torch.prod(t))\n",
    "\n",
    "# find unique \n",
    "t2 = torch.tensor([[1,2],[3,4],[3,5]])\n",
    "print(torch.unique(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb52ca-b3ba-49ae-bb01-45a29683be47",
   "metadata": {},
   "source": [
    "#### index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "acd67e7c-80c4-4493-9a48-2558ebc08053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12)\n",
      "tensor(12)\n"
     ]
    }
   ],
   "source": [
    "# index tensor1\n",
    "t_s = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "])\n",
    "print(t_s[2,3]) # line3,row4\n",
    "print(t_s[2][3]) # modify line3,row4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7fe6fbbc-b319-4712-a7d7-32415cf31a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 2, 3]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "**** [1, 3]\n",
      "tensor([[ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "#### tensor([], size=(2, 0), dtype=torch.int64)\n",
      "tensor([ 6, 10])\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]]])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# index tensor2\n",
    "# vector\n",
    "t_arr = list(range(10)) # create and array\n",
    "# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(t_arr[:]) # all elements\n",
    "print(t_arr[1:4]) # from index 1 to index 4. (contains index1 , not contains index4)\n",
    "print(t_arr[1:])# from index 1 to end. (contains index1 )\n",
    "print(t_arr[:4]) # from index 0 to 4. (not contains index4 )\n",
    "print(t_arr[:-1])# from index 0 to index(9-1) ( contains index(9-1))\n",
    "print('****',t_arr[1:4:2]) # from index 1 to index 4. (contains index1 , not contains index4, step=2)\n",
    "\n",
    "# 2d array\n",
    "t_arr2 = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "]) # create 2d array, 3rows 4cols\n",
    "print(t_arr2[1:])  # from row index1 from end (include all cols)\n",
    "print('####',t_arr2[1:3, 2:2]) # same as print(t_arr2[1:]) \n",
    "print(t_arr2[1:,1]) # second 2 cols of rows after row1\n",
    "t_arr3=t_arr2[None]\n",
    "print(t_arr3)\n",
    "print(t_arr3.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32c6f1-db79-448e-9c25-5d273f0359bb",
   "metadata": {},
   "source": [
    "### 重写鱼书里的最简单版NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364b80a-27c2-4c20-901e-5b9f30757222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e8acfb12-49f5-4925-8d9a-1c932ef1834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4983, 0.5017])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "'''\n",
    "This file is used for demo of the simplest neural network using Pytorch\n",
    "see we have 2 inputs, 2 hidden layer, each hidden layer has 3 neurons, and 2 output\n",
    "'''\n",
    "# defien neural network weights and bias\n",
    "def get_net_work_weights_bias():\n",
    "    weights_bias = {}\n",
    "    \n",
    "    weights_bias['w1'] = torch.tensor([[0.1,0.3,0.4],[0.2,0.4,0.5]],dtype=torch.float32) # 2 inputs(input), 3 neurons\n",
    "    weights_bias['b1'] = torch.tensor([0.1,0.1,0.1],dtype=torch.float32) # 3 neurons\n",
    "\n",
    "    weights_bias['w2'] = torch.tensor([[0.1,0.2,0.3],[0.2,0.3,0.4],[0.4,0.5,0.6]],dtype=torch.float32) # 3 neurons(input), 3 neurons\n",
    "    weights_bias['b2']= torch.tensor([0.2,0.2,0.2],dtype=torch.float32)# 3 neurons\n",
    "\n",
    "    weights_bias['w3'] = torch.tensor([[0.1,0.2],[0.2,0.3],[0.4,0.3]],dtype=torch.float32)# 3 neurons(input), 2 output\n",
    "    weights_bias['b3'] = torch.tensor([0.3,0.3],dtype=torch.float32)\n",
    "\n",
    "    return weights_bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    #return 1/(1+torch.exp(-x))\n",
    "    # using torch functional sigmoid\n",
    "    return torch.sigmoid(x)\n",
    "def relu(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.softmax(x,dim=0)\n",
    "\n",
    "def forward(network,x):\n",
    "    w1,w2,w3 = network['w1'],network['w2'],network['w3']\n",
    "    b1,b2,b3 = network['b1'],network['b2'],network['b3']\n",
    "\n",
    "    score1 = torch.matmul(x,w1)+b1 \n",
    "    \n",
    "    res1 = relu(score1)\n",
    "\n",
    "    score2 = torch.matmul(res1,w2)+b2\n",
    "    res2 = relu(score2)\n",
    "    \n",
    "    score3 = torch.matmul(res2,w3)+b3\n",
    "    res3 = sigmoid(score3)\n",
    "    return softmax(res3)\n",
    "\n",
    "def main():\n",
    "    network = get_net_work_weights_bias()\n",
    "    x = torch.tensor([0.1,0.2],dtype=torch.float32)\n",
    "    print(forward(network,x))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5cb45-ba3b-49d7-8bbe-0ed0f23b40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.init.kaimi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
